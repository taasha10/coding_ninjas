{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(137628, 55)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = open('kafka.txt', 'r').read().lower()\n",
    "characters = list(set(data))\n",
    "len(data), len(characters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'\\n': 53,\n",
       "  ' ': 9,\n",
       "  '!': 16,\n",
       "  '\"': 49,\n",
       "  '$': 18,\n",
       "  '%': 0,\n",
       "  \"'\": 4,\n",
       "  '(': 13,\n",
       "  ')': 39,\n",
       "  '*': 30,\n",
       "  ',': 14,\n",
       "  '-': 47,\n",
       "  '.': 25,\n",
       "  '/': 27,\n",
       "  '0': 41,\n",
       "  '1': 29,\n",
       "  '2': 6,\n",
       "  '3': 8,\n",
       "  '4': 20,\n",
       "  '5': 17,\n",
       "  '6': 19,\n",
       "  '7': 32,\n",
       "  '8': 7,\n",
       "  '9': 33,\n",
       "  ':': 26,\n",
       "  ';': 45,\n",
       "  '?': 38,\n",
       "  '@': 42,\n",
       "  'a': 43,\n",
       "  'b': 34,\n",
       "  'c': 35,\n",
       "  'd': 1,\n",
       "  'e': 3,\n",
       "  'f': 52,\n",
       "  'g': 31,\n",
       "  'h': 46,\n",
       "  'i': 51,\n",
       "  'j': 23,\n",
       "  'k': 54,\n",
       "  'l': 36,\n",
       "  'm': 40,\n",
       "  'n': 44,\n",
       "  'o': 21,\n",
       "  'p': 37,\n",
       "  'q': 24,\n",
       "  'r': 12,\n",
       "  's': 50,\n",
       "  't': 28,\n",
       "  'u': 10,\n",
       "  'v': 15,\n",
       "  'w': 22,\n",
       "  'x': 5,\n",
       "  'y': 11,\n",
       "  'z': 48,\n",
       "  'รง': 2},\n",
       " {0: '%',\n",
       "  1: 'd',\n",
       "  2: 'รง',\n",
       "  3: 'e',\n",
       "  4: \"'\",\n",
       "  5: 'x',\n",
       "  6: '2',\n",
       "  7: '8',\n",
       "  8: '3',\n",
       "  9: ' ',\n",
       "  10: 'u',\n",
       "  11: 'y',\n",
       "  12: 'r',\n",
       "  13: '(',\n",
       "  14: ',',\n",
       "  15: 'v',\n",
       "  16: '!',\n",
       "  17: '5',\n",
       "  18: '$',\n",
       "  19: '6',\n",
       "  20: '4',\n",
       "  21: 'o',\n",
       "  22: 'w',\n",
       "  23: 'j',\n",
       "  24: 'q',\n",
       "  25: '.',\n",
       "  26: ':',\n",
       "  27: '/',\n",
       "  28: 't',\n",
       "  29: '1',\n",
       "  30: '*',\n",
       "  31: 'g',\n",
       "  32: '7',\n",
       "  33: '9',\n",
       "  34: 'b',\n",
       "  35: 'c',\n",
       "  36: 'l',\n",
       "  37: 'p',\n",
       "  38: '?',\n",
       "  39: ')',\n",
       "  40: 'm',\n",
       "  41: '0',\n",
       "  42: '@',\n",
       "  43: 'a',\n",
       "  44: 'n',\n",
       "  45: ';',\n",
       "  46: 'h',\n",
       "  47: '-',\n",
       "  48: 'z',\n",
       "  49: '\"',\n",
       "  50: 's',\n",
       "  51: 'i',\n",
       "  52: 'f',\n",
       "  53: '\\n',\n",
       "  54: 'k'})"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "char_to_index = {ch:i for i,ch in enumerate(characters)}\n",
    "index_to_char = {i:ch for i,ch in enumerate(characters)}\n",
    "\n",
    "char_to_index, index_to_char"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "hidden_units = 100\n",
    "learning_rate = 0.1\n",
    "length_seq = 20\n",
    "vocab_size = len(characters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "Wxh = np.random.randn(hidden_units, vocab_size)* 0.01\n",
    "Whh = np.random.randn(hidden_units, hidden_units)* 0.01\n",
    "Why = np.random.randn(vocab_size, hidden_units)* 0.01\n",
    "bh = np.zeros((hidden_units, 1))\n",
    "by = np.zeros((vocab_size, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "inp = np.zeros((vocab_size, 1))\n",
    "inp[char_to_index['a']] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def next_char(current_char, h_prev):\n",
    "    current_input = np.zeros((vocab_size, 1))\n",
    "    current_input[char_to_index[current_char]] = 1\n",
    "    hidden_output = np.tanh(bh + np.dot(Wxh, current_input) + np.dot(Whh, h_prev))\n",
    "    output = np.dot(Why, hidden_output) + by\n",
    "    prob = np.exp(output)/np.sum(np.exp(output))\n",
    "    max_index = np.argmax(prob)\n",
    "    output_char = index_to_char[max_index]\n",
    "    return output_char, hidden_output\n",
    "                  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(inputs, targets, h_prev):\n",
    "    #h_prev = np.zeros((hidden_units, 1))\n",
    "    xs, hs, os, ps = {},{},{},{}\n",
    "    hs[-1] = np.copy(h_prev)\n",
    "    loss = 0\n",
    "    for i in range(len(inputs)):\n",
    "        xs[i] = np.zeros((vocab_size, 1))\n",
    "        xs[i][char_to_index[inputs[i]]] = 1\n",
    "        hs[i] = np.tanh(bh + np.dot(Wxh, xs[i]) + np.dot(Whh, hs[i - 1]))\n",
    "        os[i] = np.dot(Why, hs[i]) + by\n",
    "        ps[i] = np.exp(os[i])/np.sum(np.exp(os[i]))\n",
    "        loss += -np.log(ps[i][char_to_index[targets[i]], 0]) # TODO\n",
    "    # backward pass: compute gradients going backwards    \n",
    "    #initalize vectors for gradient values for each set of weights \n",
    "    dWxh, dWhh, dWhy = np.zeros_like(Wxh), np.zeros_like(Whh), np.zeros_like(Why)\n",
    "    dbh, dby = np.zeros_like(bh), np.zeros_like(by)\n",
    "    dhnext = np.zeros_like(hs[0])\n",
    "    for t in reversed(range(len(inputs))):\n",
    "        dy = np.copy(ps[t])\n",
    "        dy[char_to_index[targets[t]]] -= 1 # backprop into y  \n",
    "        dWhy += np.dot(dy, hs[t].T)\n",
    "        dby += dy\n",
    "        dh = np.dot(Why.T, dy) + dhnext # backprop into h                                                                                                                                         \n",
    "        dhraw = (1 - hs[t] * hs[t]) * dh # backprop through tanh nonlinearity                                                                                                                     \n",
    "        dbh += dhraw #derivative of hidden bias\n",
    "        dWxh += np.dot(dhraw, xs[t].T) #derivative of input to hidden layer weight\n",
    "        dWhh += np.dot(dhraw, hs[t-1].T) #derivative of hidden layer to hidden layer weight\n",
    "        dhnext = np.dot(Whh.T, dhraw) \n",
    "    for dparam in [dWxh, dWhh, dWhy, dbh, dby]:\n",
    "        np.clip(dparam, -5, 5, out=dparam) # clip to mitigate exploding gradients                                                                                                                 \n",
    "    return loss, dWxh, dWhh, dWhy, dbh, dby, hs[len(inputs)-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def generate_sentence(seed, n, h_prev):\n",
    "    character = seed\n",
    "    sentence = \"\" + seed\n",
    "    for i in range(n):\n",
    "        character, h_prev = next_char(character, h_prev)\n",
    "        sentence += character\n",
    "    print(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 0, loss: 80.101259\n",
      "iter 1000, loss: 60.135291\n",
      "iter 2000, loss: 50.040702\n",
      "iter 3000, loss: 45.278683\n",
      "iter 4000, loss: 43.614622\n",
      "iter 5000, loss: 42.510850\n",
      "iter 6000, loss: 43.863524\n",
      "iter 7000, loss: 48.037524\n",
      "iter 8000, loss: 45.089570\n",
      "iter 9000, loss: 42.701370\n",
      "iter 10000, loss: 41.542882\n",
      "iter 11000, loss: 41.232602\n",
      "iter 12000, loss: 40.941031\n",
      "iter 13000, loss: 43.079837\n",
      "iter 14000, loss: 45.938388\n",
      "iter 15000, loss: 43.604287\n",
      "iter 16000, loss: 41.643439\n",
      "iter 17000, loss: 40.564354\n",
      "iter 18000, loss: 40.459586\n",
      "iter 19000, loss: 40.341264\n",
      "iter 20000, loss: 42.622351\n",
      "iter 21000, loss: 44.911514\n",
      "iter 22000, loss: 42.595618\n",
      "iter 23000, loss: 40.967321\n",
      "iter 24000, loss: 39.944500\n",
      "iter 25000, loss: 39.775725\n",
      "iter 26000, loss: 39.818716\n",
      "iter 27000, loss: 42.714002\n",
      "iter 28000, loss: 43.954923\n",
      "iter 29000, loss: 42.148569\n",
      "iter 30000, loss: 40.371060\n",
      "iter 31000, loss: 39.427729\n",
      "iter 32000, loss: 39.261400\n",
      "iter 33000, loss: 39.404866\n",
      "iter 34000, loss: 42.916981\n",
      "iter 35000, loss: 43.110238\n",
      "iter 36000, loss: 41.593384\n",
      "iter 37000, loss: 39.869835\n",
      "iter 38000, loss: 39.076559\n",
      "iter 39000, loss: 38.829093\n",
      "iter 40000, loss: 39.158866\n"
     ]
    }
   ],
   "source": [
    "n, p = 0, 0\n",
    "mWxh, mWhh, mWhy = np.zeros_like(Wxh), np.zeros_like(Whh), np.zeros_like(Why)\n",
    "mbh, mby = np.zeros_like(bh), np.zeros_like(by) # memory variables for Adagrad                                                                                                                \n",
    "smooth_loss = -np.log(1.0/vocab_size)*length_seq # loss at iteration 0           \n",
    "while n<=1000*40:\n",
    "    # prepare inputs (we're sweeping from left to right in steps seq_length long)\n",
    "    # check \"How to feed the loss function to see how this part works\n",
    "    if p+length_seq+1 >= len(data) or n == 0:\n",
    "        hprev = np.zeros((hidden_units,1)) # reset RNN memory                                                                                                                                      \n",
    "        p = 0 # go from start of data                                                                                                                                                             \n",
    "    \n",
    "    inputs = [ch for ch in data[p:p+length_seq]]\n",
    "    targets = [ch for ch in data[p+1:p+length_seq+1]]\n",
    "\n",
    "    # forward seq_length characters through the net and fetch gradient                                                                                                                          \n",
    "    loss, dWxh, dWhh, dWhy, dbh, dby, hprev = train(inputs, targets, hprev)\n",
    "    smooth_loss = smooth_loss * 0.999 + loss * 0.001\n",
    "\n",
    "    # sample from the model now and then                                                                                                                                                        \n",
    "    if n % 1000 == 0:\n",
    "        print('iter %d, loss: %f' % (n, smooth_loss)) # print progress\n",
    "        #sample(hprev, inputs[0], 200)\n",
    "\n",
    "    # perform parameter update with Adagrad                                                                                                                                                     \n",
    "    for param, dparam, mem in zip([Wxh, Whh, Why, bh, by],\n",
    "                                [dWxh, dWhh, dWhy, dbh, dby],\n",
    "                                [mWxh, mWhh, mWhy, mbh, mby]):\n",
    "        mem += dparam * dparam\n",
    "        current_learning = learning_rate/np.sqrt(mem + 1e-8)\n",
    "        param += -current_learning * dparam  # adagrad update                                                                                                                   \n",
    "\n",
    "    p += length_seq # move data pointer                                                                                                                                                         \n",
    "    n += 1 # iteration counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a75;ve*auf*auf*auf*auf*auf*auf*auf*auf*auf*auf*auf*\n"
     ]
    }
   ],
   "source": [
    "generate_sentence('a', 50, hprev)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
