{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn import datasets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RangeIndex(start=0, stop=13, step=1)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<bound method NDFrame.head of            0     1      2    3      4      5      6       7     8      9   \\\n",
       "0     0.00632  18.0   2.31  0.0  0.538  6.575   65.2  4.0900   1.0  296.0   \n",
       "1     0.02731   0.0   7.07  0.0  0.469  6.421   78.9  4.9671   2.0  242.0   \n",
       "2     0.02729   0.0   7.07  0.0  0.469  7.185   61.1  4.9671   2.0  242.0   \n",
       "3     0.03237   0.0   2.18  0.0  0.458  6.998   45.8  6.0622   3.0  222.0   \n",
       "4     0.06905   0.0   2.18  0.0  0.458  7.147   54.2  6.0622   3.0  222.0   \n",
       "5     0.02985   0.0   2.18  0.0  0.458  6.430   58.7  6.0622   3.0  222.0   \n",
       "6     0.08829  12.5   7.87  0.0  0.524  6.012   66.6  5.5605   5.0  311.0   \n",
       "7     0.14455  12.5   7.87  0.0  0.524  6.172   96.1  5.9505   5.0  311.0   \n",
       "8     0.21124  12.5   7.87  0.0  0.524  5.631  100.0  6.0821   5.0  311.0   \n",
       "9     0.17004  12.5   7.87  0.0  0.524  6.004   85.9  6.5921   5.0  311.0   \n",
       "10    0.22489  12.5   7.87  0.0  0.524  6.377   94.3  6.3467   5.0  311.0   \n",
       "11    0.11747  12.5   7.87  0.0  0.524  6.009   82.9  6.2267   5.0  311.0   \n",
       "12    0.09378  12.5   7.87  0.0  0.524  5.889   39.0  5.4509   5.0  311.0   \n",
       "13    0.62976   0.0   8.14  0.0  0.538  5.949   61.8  4.7075   4.0  307.0   \n",
       "14    0.63796   0.0   8.14  0.0  0.538  6.096   84.5  4.4619   4.0  307.0   \n",
       "15    0.62739   0.0   8.14  0.0  0.538  5.834   56.5  4.4986   4.0  307.0   \n",
       "16    1.05393   0.0   8.14  0.0  0.538  5.935   29.3  4.4986   4.0  307.0   \n",
       "17    0.78420   0.0   8.14  0.0  0.538  5.990   81.7  4.2579   4.0  307.0   \n",
       "18    0.80271   0.0   8.14  0.0  0.538  5.456   36.6  3.7965   4.0  307.0   \n",
       "19    0.72580   0.0   8.14  0.0  0.538  5.727   69.5  3.7965   4.0  307.0   \n",
       "20    1.25179   0.0   8.14  0.0  0.538  5.570   98.1  3.7979   4.0  307.0   \n",
       "21    0.85204   0.0   8.14  0.0  0.538  5.965   89.2  4.0123   4.0  307.0   \n",
       "22    1.23247   0.0   8.14  0.0  0.538  6.142   91.7  3.9769   4.0  307.0   \n",
       "23    0.98843   0.0   8.14  0.0  0.538  5.813  100.0  4.0952   4.0  307.0   \n",
       "24    0.75026   0.0   8.14  0.0  0.538  5.924   94.1  4.3996   4.0  307.0   \n",
       "25    0.84054   0.0   8.14  0.0  0.538  5.599   85.7  4.4546   4.0  307.0   \n",
       "26    0.67191   0.0   8.14  0.0  0.538  5.813   90.3  4.6820   4.0  307.0   \n",
       "27    0.95577   0.0   8.14  0.0  0.538  6.047   88.8  4.4534   4.0  307.0   \n",
       "28    0.77299   0.0   8.14  0.0  0.538  6.495   94.4  4.4547   4.0  307.0   \n",
       "29    1.00245   0.0   8.14  0.0  0.538  6.674   87.3  4.2390   4.0  307.0   \n",
       "..        ...   ...    ...  ...    ...    ...    ...     ...   ...    ...   \n",
       "476   4.87141   0.0  18.10  0.0  0.614  6.484   93.6  2.3053  24.0  666.0   \n",
       "477  15.02340   0.0  18.10  0.0  0.614  5.304   97.3  2.1007  24.0  666.0   \n",
       "478  10.23300   0.0  18.10  0.0  0.614  6.185   96.7  2.1705  24.0  666.0   \n",
       "479  14.33370   0.0  18.10  0.0  0.614  6.229   88.0  1.9512  24.0  666.0   \n",
       "480   5.82401   0.0  18.10  0.0  0.532  6.242   64.7  3.4242  24.0  666.0   \n",
       "481   5.70818   0.0  18.10  0.0  0.532  6.750   74.9  3.3317  24.0  666.0   \n",
       "482   5.73116   0.0  18.10  0.0  0.532  7.061   77.0  3.4106  24.0  666.0   \n",
       "483   2.81838   0.0  18.10  0.0  0.532  5.762   40.3  4.0983  24.0  666.0   \n",
       "484   2.37857   0.0  18.10  0.0  0.583  5.871   41.9  3.7240  24.0  666.0   \n",
       "485   3.67367   0.0  18.10  0.0  0.583  6.312   51.9  3.9917  24.0  666.0   \n",
       "486   5.69175   0.0  18.10  0.0  0.583  6.114   79.8  3.5459  24.0  666.0   \n",
       "487   4.83567   0.0  18.10  0.0  0.583  5.905   53.2  3.1523  24.0  666.0   \n",
       "488   0.15086   0.0  27.74  0.0  0.609  5.454   92.7  1.8209   4.0  711.0   \n",
       "489   0.18337   0.0  27.74  0.0  0.609  5.414   98.3  1.7554   4.0  711.0   \n",
       "490   0.20746   0.0  27.74  0.0  0.609  5.093   98.0  1.8226   4.0  711.0   \n",
       "491   0.10574   0.0  27.74  0.0  0.609  5.983   98.8  1.8681   4.0  711.0   \n",
       "492   0.11132   0.0  27.74  0.0  0.609  5.983   83.5  2.1099   4.0  711.0   \n",
       "493   0.17331   0.0   9.69  0.0  0.585  5.707   54.0  2.3817   6.0  391.0   \n",
       "494   0.27957   0.0   9.69  0.0  0.585  5.926   42.6  2.3817   6.0  391.0   \n",
       "495   0.17899   0.0   9.69  0.0  0.585  5.670   28.8  2.7986   6.0  391.0   \n",
       "496   0.28960   0.0   9.69  0.0  0.585  5.390   72.9  2.7986   6.0  391.0   \n",
       "497   0.26838   0.0   9.69  0.0  0.585  5.794   70.6  2.8927   6.0  391.0   \n",
       "498   0.23912   0.0   9.69  0.0  0.585  6.019   65.3  2.4091   6.0  391.0   \n",
       "499   0.17783   0.0   9.69  0.0  0.585  5.569   73.5  2.3999   6.0  391.0   \n",
       "500   0.22438   0.0   9.69  0.0  0.585  6.027   79.7  2.4982   6.0  391.0   \n",
       "501   0.06263   0.0  11.93  0.0  0.573  6.593   69.1  2.4786   1.0  273.0   \n",
       "502   0.04527   0.0  11.93  0.0  0.573  6.120   76.7  2.2875   1.0  273.0   \n",
       "503   0.06076   0.0  11.93  0.0  0.573  6.976   91.0  2.1675   1.0  273.0   \n",
       "504   0.10959   0.0  11.93  0.0  0.573  6.794   89.3  2.3889   1.0  273.0   \n",
       "505   0.04741   0.0  11.93  0.0  0.573  6.030   80.8  2.5050   1.0  273.0   \n",
       "\n",
       "       10      11     12  \n",
       "0    15.3  396.90   4.98  \n",
       "1    17.8  396.90   9.14  \n",
       "2    17.8  392.83   4.03  \n",
       "3    18.7  394.63   2.94  \n",
       "4    18.7  396.90   5.33  \n",
       "5    18.7  394.12   5.21  \n",
       "6    15.2  395.60  12.43  \n",
       "7    15.2  396.90  19.15  \n",
       "8    15.2  386.63  29.93  \n",
       "9    15.2  386.71  17.10  \n",
       "10   15.2  392.52  20.45  \n",
       "11   15.2  396.90  13.27  \n",
       "12   15.2  390.50  15.71  \n",
       "13   21.0  396.90   8.26  \n",
       "14   21.0  380.02  10.26  \n",
       "15   21.0  395.62   8.47  \n",
       "16   21.0  386.85   6.58  \n",
       "17   21.0  386.75  14.67  \n",
       "18   21.0  288.99  11.69  \n",
       "19   21.0  390.95  11.28  \n",
       "20   21.0  376.57  21.02  \n",
       "21   21.0  392.53  13.83  \n",
       "22   21.0  396.90  18.72  \n",
       "23   21.0  394.54  19.88  \n",
       "24   21.0  394.33  16.30  \n",
       "25   21.0  303.42  16.51  \n",
       "26   21.0  376.88  14.81  \n",
       "27   21.0  306.38  17.28  \n",
       "28   21.0  387.94  12.80  \n",
       "29   21.0  380.23  11.98  \n",
       "..    ...     ...    ...  \n",
       "476  20.2  396.21  18.68  \n",
       "477  20.2  349.48  24.91  \n",
       "478  20.2  379.70  18.03  \n",
       "479  20.2  383.32  13.11  \n",
       "480  20.2  396.90  10.74  \n",
       "481  20.2  393.07   7.74  \n",
       "482  20.2  395.28   7.01  \n",
       "483  20.2  392.92  10.42  \n",
       "484  20.2  370.73  13.34  \n",
       "485  20.2  388.62  10.58  \n",
       "486  20.2  392.68  14.98  \n",
       "487  20.2  388.22  11.45  \n",
       "488  20.1  395.09  18.06  \n",
       "489  20.1  344.05  23.97  \n",
       "490  20.1  318.43  29.68  \n",
       "491  20.1  390.11  18.07  \n",
       "492  20.1  396.90  13.35  \n",
       "493  19.2  396.90  12.01  \n",
       "494  19.2  396.90  13.59  \n",
       "495  19.2  393.29  17.60  \n",
       "496  19.2  396.90  21.14  \n",
       "497  19.2  396.90  14.10  \n",
       "498  19.2  396.90  12.92  \n",
       "499  19.2  395.77  15.10  \n",
       "500  19.2  396.90  14.33  \n",
       "501  21.0  391.99   9.67  \n",
       "502  21.0  396.90   9.08  \n",
       "503  21.0  396.90   5.64  \n",
       "504  21.0  393.45   6.48  \n",
       "505  21.0  396.90   7.88  \n",
       "\n",
       "[506 rows x 13 columns]>"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr = LinearRegression()\n",
    "boston = datasets.load_boston()\n",
    "df = pd.DataFrame(boston.data)\n",
    "print(df.columns)\n",
    "df.head\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['data', 'target', 'feature_names', 'DESCR'])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "boston.keys()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['CRIM', 'ZN', 'INDUS', 'CHAS', 'NOX', 'RM', 'AGE', 'DIS', 'RAD',\n",
       "       'TAX', 'PTRATIO', 'B', 'LSTAT'],\n",
       "      dtype='<U7')"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "boston['feature_names']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Boston House Prices dataset\\n===========================\\n\\nNotes\\n------\\nData Set Characteristics:  \\n\\n    :Number of Instances: 506 \\n\\n    :Number of Attributes: 13 numeric/categorical predictive\\n    \\n    :Median Value (attribute 14) is usually the target\\n\\n    :Attribute Information (in order):\\n        - CRIM     per capita crime rate by town\\n        - ZN       proportion of residential land zoned for lots over 25,000 sq.ft.\\n        - INDUS    proportion of non-retail business acres per town\\n        - CHAS     Charles River dummy variable (= 1 if tract bounds river; 0 otherwise)\\n        - NOX      nitric oxides concentration (parts per 10 million)\\n        - RM       average number of rooms per dwelling\\n        - AGE      proportion of owner-occupied units built prior to 1940\\n        - DIS      weighted distances to five Boston employment centres\\n        - RAD      index of accessibility to radial highways\\n        - TAX      full-value property-tax rate per $10,000\\n        - PTRATIO  pupil-teacher ratio by town\\n        - B        1000(Bk - 0.63)^2 where Bk is the proportion of blacks by town\\n        - LSTAT    % lower status of the population\\n        - MEDV     Median value of owner-occupied homes in $1000's\\n\\n    :Missing Attribute Values: None\\n\\n    :Creator: Harrison, D. and Rubinfeld, D.L.\\n\\nThis is a copy of UCI ML housing dataset.\\nhttp://archive.ics.uci.edu/ml/datasets/Housing\\n\\n\\nThis dataset was taken from the StatLib library which is maintained at Carnegie Mellon University.\\n\\nThe Boston house-price data of Harrison, D. and Rubinfeld, D.L. 'Hedonic\\nprices and the demand for clean air', J. Environ. Economics & Management,\\nvol.5, 81-102, 1978.   Used in Belsley, Kuh & Welsch, 'Regression diagnostics\\n...', Wiley, 1980.   N.B. Various transformations are used in the table on\\npages 244-261 of the latter.\\n\\nThe Boston house-price data has been used in many machine learning papers that address regression\\nproblems.   \\n     \\n**References**\\n\\n   - Belsley, Kuh & Welsch, 'Regression diagnostics: Identifying Influential Data and Sources of Collinearity', Wiley, 1980. 244-261.\\n   - Quinlan,R. (1993). Combining Instance-Based and Model-Based Learning. In Proceedings on the Tenth International Conference of Machine Learning, 236-243, University of Massachusetts, Amherst. Morgan Kaufmann.\\n   - many more! (see http://archive.ics.uci.edu/ml/datasets/Housing)\\n\""
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "boston['DESCR']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           0     1      2    3       4      5      6       7     8      9   \\\n",
      "402   9.59571   0.0  18.10  0.0  0.6930  6.404  100.0  1.6390  24.0  666.0   \n",
      "139   0.54452   0.0  21.89  0.0  0.6240  6.151   97.9  1.6687   4.0  437.0   \n",
      "450   6.71772   0.0  18.10  0.0  0.7130  6.749   92.6  2.3236  24.0  666.0   \n",
      "7     0.14455  12.5   7.87  0.0  0.5240  6.172   96.1  5.9505   5.0  311.0   \n",
      "196   0.04011  80.0   1.52  0.0  0.4040  7.287   34.1  7.3090   2.0  329.0   \n",
      "136   0.32264   0.0  21.89  0.0  0.6240  5.942   93.5  1.9669   4.0  437.0   \n",
      "503   0.06076   0.0  11.93  0.0  0.5730  6.976   91.0  2.1675   1.0  273.0   \n",
      "71    0.15876   0.0  10.81  0.0  0.4130  5.961   17.5  5.2873   4.0  305.0   \n",
      "397   7.67202   0.0  18.10  0.0  0.6930  5.747   98.9  1.6334  24.0  666.0   \n",
      "111   0.10084   0.0  10.01  0.0  0.5470  6.715   81.6  2.6775   6.0  432.0   \n",
      "34    1.61282   0.0   8.14  0.0  0.5380  6.096   96.9  3.7598   4.0  307.0   \n",
      "268   0.54050  20.0   3.97  0.0  0.5750  7.470   52.6  2.8720   5.0  264.0   \n",
      "455   4.75237   0.0  18.10  0.0  0.7130  6.525   86.5  2.4358  24.0  666.0   \n",
      "108   0.12802   0.0   8.56  0.0  0.5200  6.474   97.1  2.4329   5.0  384.0   \n",
      "486   5.69175   0.0  18.10  0.0  0.5830  6.114   79.8  3.5459  24.0  666.0   \n",
      "68    0.13554  12.5   6.07  0.0  0.4090  5.594   36.8  6.4980   4.0  345.0   \n",
      "47    0.22927   0.0   6.91  0.0  0.4480  6.030   85.5  5.6894   3.0  233.0   \n",
      "404  41.52920   0.0  18.10  0.0  0.6930  5.531   85.4  1.6074  24.0  666.0   \n",
      "372   8.26725   0.0  18.10  1.0  0.6680  5.875   89.6  1.1296  24.0  666.0   \n",
      "364   3.47428   0.0  18.10  1.0  0.7180  8.780   82.9  1.9047  24.0  666.0   \n",
      "381  15.87440   0.0  18.10  0.0  0.6710  6.545   99.1  1.5192  24.0  666.0   \n",
      "84    0.05059   0.0   4.49  0.0  0.4490  6.389   48.0  4.7794   3.0  247.0   \n",
      "348   0.01501  80.0   2.01  0.0  0.4350  6.635   29.7  8.3440   4.0  280.0   \n",
      "451   5.44114   0.0  18.10  0.0  0.7130  6.655   98.2  2.3552  24.0  666.0   \n",
      "0     0.00632  18.0   2.31  0.0  0.5380  6.575   65.2  4.0900   1.0  296.0   \n",
      "112   0.12329   0.0  10.01  0.0  0.5470  5.913   92.9  2.3534   6.0  432.0   \n",
      "476   4.87141   0.0  18.10  0.0  0.6140  6.484   93.6  2.3053  24.0  666.0   \n",
      "363   4.22239   0.0  18.10  1.0  0.7700  5.803   89.0  1.9047  24.0  666.0   \n",
      "15    0.62739   0.0   8.14  0.0  0.5380  5.834   56.5  4.4986   4.0  307.0   \n",
      "114   0.14231   0.0  10.01  0.0  0.5470  6.254   84.2  2.2565   6.0  432.0   \n",
      "..        ...   ...    ...  ...     ...    ...    ...     ...   ...    ...   \n",
      "80    0.04113  25.0   4.86  0.0  0.4260  6.727   33.5  5.4007   4.0  281.0   \n",
      "40    0.03359  75.0   2.95  0.0  0.4280  7.024   15.8  5.4011   3.0  252.0   \n",
      "240   0.11329  30.0   4.93  0.0  0.4280  6.897   54.3  6.3361   6.0  300.0   \n",
      "258   0.66351  20.0   3.97  0.0  0.6470  7.333  100.0  1.8946   5.0  264.0   \n",
      "140   0.29090   0.0  21.89  0.0  0.6240  6.174   93.6  1.6119   4.0  437.0   \n",
      "106   0.17120   0.0   8.56  0.0  0.5200  5.836   91.9  2.2110   5.0  384.0   \n",
      "69    0.12816  12.5   6.07  0.0  0.4090  5.885   33.0  6.4980   4.0  345.0   \n",
      "96    0.11504   0.0   2.89  0.0  0.4450  6.163   69.6  3.4952   2.0  276.0   \n",
      "54    0.01360  75.0   4.00  0.0  0.4100  5.888   47.6  7.3197   3.0  469.0   \n",
      "178   0.06642   0.0   4.05  0.0  0.5100  6.860   74.4  2.9153   5.0  296.0   \n",
      "267   0.57834  20.0   3.97  0.0  0.5750  8.297   67.0  2.4216   5.0  264.0   \n",
      "81    0.04462  25.0   4.86  0.0  0.4260  6.619   70.4  5.4007   4.0  281.0   \n",
      "393   8.64476   0.0  18.10  0.0  0.6930  6.193   92.6  1.7912  24.0  666.0   \n",
      "64    0.01951  17.5   1.38  0.0  0.4161  7.104   59.5  9.2229   3.0  216.0   \n",
      "138   0.24980   0.0  21.89  0.0  0.6240  5.857   98.2  1.6686   4.0  437.0   \n",
      "261   0.53412  20.0   3.97  0.0  0.6470  7.520   89.4  2.1398   5.0  264.0   \n",
      "50    0.08873  21.0   5.64  0.0  0.4390  5.963   45.7  6.8147   4.0  243.0   \n",
      "145   2.37934   0.0  19.58  0.0  0.8710  6.130  100.0  1.4191   5.0  403.0   \n",
      "233   0.33147   0.0   6.20  0.0  0.5070  8.247   70.4  3.6519   8.0  307.0   \n",
      "46    0.18836   0.0   6.91  0.0  0.4480  5.786   33.3  5.1004   3.0  233.0   \n",
      "91    0.03932   0.0   3.41  0.0  0.4890  6.405   73.9  3.0921   2.0  270.0   \n",
      "174   0.08447   0.0   4.05  0.0  0.5100  5.859   68.7  2.7019   5.0  296.0   \n",
      "327   0.24103   0.0   7.38  0.0  0.4930  6.083   43.7  5.4159   5.0  287.0   \n",
      "148   2.33099   0.0  19.58  0.0  0.8710  5.186   93.8  1.5296   5.0  403.0   \n",
      "438  13.67810   0.0  18.10  0.0  0.7400  5.935   87.9  1.8206  24.0  666.0   \n",
      "370   6.53876   0.0  18.10  1.0  0.6310  7.016   97.5  1.2024  24.0  666.0   \n",
      "76    0.10153   0.0  12.83  0.0  0.4370  6.279   74.5  4.0522   5.0  398.0   \n",
      "332   0.03466  35.0   6.06  0.0  0.4379  6.031   23.3  6.6407   1.0  304.0   \n",
      "452   5.09017   0.0  18.10  0.0  0.7130  6.297   91.8  2.3682  24.0  666.0   \n",
      "126   0.38735   0.0  25.65  0.0  0.5810  5.613   95.6  1.7572   2.0  188.0   \n",
      "\n",
      "       10      11     12  \n",
      "402  20.2  376.11  20.31  \n",
      "139  21.2  396.90  18.46  \n",
      "450  20.2    0.32  17.44  \n",
      "7    15.2  396.90  19.15  \n",
      "196  12.6  396.90   4.08  \n",
      "136  21.2  378.25  16.90  \n",
      "503  21.0  396.90   5.64  \n",
      "71   19.2  376.94   9.88  \n",
      "397  20.2  393.10  19.92  \n",
      "111  17.8  395.59  10.16  \n",
      "34   21.0  248.31  20.34  \n",
      "268  13.0  390.30   3.16  \n",
      "455  20.2   50.92  18.13  \n",
      "108  20.9  395.24  12.27  \n",
      "486  20.2  392.68  14.98  \n",
      "68   18.9  396.90  13.09  \n",
      "47   17.9  392.74  18.80  \n",
      "404  20.2  329.46  27.38  \n",
      "372  20.2  347.88   8.88  \n",
      "364  20.2  354.55   5.29  \n",
      "381  20.2  396.90  21.08  \n",
      "84   18.5  396.90   9.62  \n",
      "348  17.0  390.94   5.99  \n",
      "451  20.2  355.29  17.73  \n",
      "0    15.3  396.90   4.98  \n",
      "112  17.8  394.95  16.21  \n",
      "476  20.2  396.21  18.68  \n",
      "363  20.2  353.04  14.64  \n",
      "15   21.0  395.62   8.47  \n",
      "114  17.8  388.74  10.45  \n",
      "..    ...     ...    ...  \n",
      "80   19.0  396.90   5.29  \n",
      "40   18.3  395.62   1.98  \n",
      "240  16.6  391.25  11.38  \n",
      "258  13.0  383.29   7.79  \n",
      "140  21.2  388.08  24.16  \n",
      "106  20.9  395.67  18.66  \n",
      "69   18.9  396.90   8.79  \n",
      "96   18.0  391.83  11.34  \n",
      "54   21.1  396.90  14.80  \n",
      "178  16.6  391.27   6.92  \n",
      "267  13.0  384.54   7.44  \n",
      "81   19.0  395.63   7.22  \n",
      "393  20.2  396.90  15.17  \n",
      "64   18.6  393.24   8.05  \n",
      "138  21.2  392.04  21.32  \n",
      "261  13.0  388.37   7.26  \n",
      "50   16.8  395.56  13.45  \n",
      "145  14.7  172.91  27.80  \n",
      "233  17.4  378.95   3.95  \n",
      "46   17.9  396.90  14.15  \n",
      "91   17.8  393.55   8.20  \n",
      "174  16.6  393.23   9.64  \n",
      "327  19.6  396.90  12.79  \n",
      "148  14.7  356.99  28.32  \n",
      "438  20.2   68.95  34.02  \n",
      "370  20.2  392.05   2.96  \n",
      "76   18.7  373.66  11.97  \n",
      "332  16.9  362.25   7.83  \n",
      "452  20.2  385.09  17.27  \n",
      "126  19.1  359.29  27.26  \n",
      "\n",
      "[404 rows x 13 columns]\n"
     ]
    }
   ],
   "source": [
    "from sklearn import cross_validation as cv\n",
    "x_train, x_test, y_train, y_test = cv.train_test_split(df,boston.target,test_size = 0.2)\n",
    "lr.fit(x_train,y_train)\n",
    "print(x_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.57700717105623434"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr.score(x_test,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x_train.reset_index(drop=False,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[4.55587,\n",
       " 0.97617,\n",
       " 0.04544,\n",
       " 1.27346,\n",
       " 2.44668,\n",
       " 1.05393,\n",
       " 0.01951,\n",
       " 5.87205,\n",
       " 8.26725,\n",
       " 0.14932,\n",
       " 0.09378,\n",
       " 0.06911,\n",
       " 0.09266,\n",
       " 9.39063,\n",
       " 9.51363,\n",
       " 14.3337,\n",
       " 0.2909,\n",
       " 14.4383,\n",
       " 0.08447,\n",
       " 7.36711,\n",
       " 0.04113,\n",
       " 0.05023,\n",
       " 45.7461,\n",
       " 2.73397,\n",
       " 0.03738,\n",
       " 0.08265,\n",
       " 1.19294,\n",
       " 0.05644,\n",
       " 0.17446,\n",
       " 0.13642,\n",
       " 0.82526,\n",
       " 9.18702,\n",
       " 6.65492,\n",
       " 1.13081,\n",
       " 0.0136,\n",
       " 1.38799,\n",
       " 0.03237,\n",
       " 0.33045,\n",
       " 0.1712,\n",
       " 0.08664,\n",
       " 10.0623,\n",
       " 0.15876,\n",
       " 0.02177,\n",
       " 18.4982,\n",
       " 0.88125,\n",
       " 0.25356,\n",
       " 0.76162,\n",
       " 0.04379,\n",
       " 7.05042,\n",
       " 4.0974,\n",
       " 0.55007,\n",
       " 0.02875,\n",
       " 0.03615,\n",
       " 8.79212,\n",
       " 3.47428,\n",
       " 0.18337,\n",
       " 0.1396,\n",
       " 0.0795,\n",
       " 1.49632,\n",
       " 0.07244,\n",
       " 0.03306,\n",
       " 0.32982,\n",
       " 0.11069,\n",
       " 0.04527,\n",
       " 7.40389,\n",
       " 0.22188,\n",
       " 0.05515,\n",
       " 0.05479,\n",
       " 0.11132,\n",
       " 8.15174,\n",
       " 2.81838,\n",
       " 0.01381,\n",
       " 0.36894,\n",
       " 3.83684,\n",
       " 0.19073,\n",
       " 0.03705,\n",
       " 0.25915,\n",
       " 1.34284,\n",
       " 0.21409,\n",
       " 0.16439,\n",
       " 0.09252,\n",
       " 0.08829,\n",
       " 0.62739,\n",
       " 1.62864,\n",
       " 0.22969,\n",
       " 0.07013,\n",
       " 8.71675,\n",
       " 0.63796,\n",
       " 0.02187,\n",
       " 0.17505,\n",
       " 0.12932,\n",
       " 0.07165,\n",
       " 0.26938,\n",
       " 0.06211,\n",
       " 0.26169,\n",
       " 0.01432,\n",
       " 8.98296,\n",
       " 2.37934,\n",
       " 0.47547,\n",
       " 1.15172,\n",
       " 13.9134,\n",
       " 5.66637,\n",
       " 0.13262,\n",
       " 0.32543,\n",
       " 0.06076,\n",
       " 0.12083,\n",
       " 3.69695,\n",
       " 15.0234,\n",
       " 3.1636,\n",
       " 0.03932,\n",
       " 0.22876,\n",
       " 0.14455,\n",
       " 0.02731,\n",
       " 88.9762,\n",
       " 0.0578,\n",
       " 0.11504,\n",
       " 0.26363,\n",
       " 0.06466,\n",
       " 7.52601,\n",
       " 0.21124,\n",
       " 0.00906,\n",
       " 0.07151,\n",
       " 0.14052,\n",
       " 0.65665,\n",
       " 0.17171,\n",
       " 0.02009,\n",
       " 0.08014,\n",
       " 0.6718,\n",
       " 38.3518,\n",
       " 0.1029,\n",
       " 0.12329,\n",
       " 0.31533,\n",
       " 12.0482,\n",
       " 0.02985,\n",
       " 0.05646,\n",
       " 1.35472,\n",
       " 25.9406,\n",
       " 0.05602,\n",
       " 0.29916,\n",
       " 0.01538,\n",
       " 0.13117,\n",
       " 11.1604,\n",
       " 13.3598,\n",
       " 0.44791,\n",
       " 0.10469,\n",
       " 20.7162,\n",
       " 0.02055,\n",
       " 23.6482,\n",
       " 0.54011,\n",
       " 0.12757,\n",
       " 0.66351,\n",
       " 4.34879,\n",
       " 13.0751,\n",
       " 5.58107,\n",
       " 0.03548,\n",
       " 5.44114,\n",
       " 7.75223,\n",
       " 0.18159,\n",
       " 22.0511,\n",
       " 0.03502,\n",
       " 0.03113,\n",
       " 1.46336,\n",
       " 14.2362,\n",
       " 0.17134,\n",
       " 3.69311,\n",
       " 0.08244,\n",
       " 0.01439,\n",
       " 7.99248,\n",
       " 0.02729,\n",
       " 0.02899,\n",
       " 4.54192,\n",
       " 2.924,\n",
       " 0.75026,\n",
       " 4.75237,\n",
       " 4.81213,\n",
       " 67.9208,\n",
       " 0.09849,\n",
       " 0.13554,\n",
       " 0.35233,\n",
       " 0.11747,\n",
       " 0.14866,\n",
       " 0.19657,\n",
       " 0.25387,\n",
       " 2.15505,\n",
       " 0.6147,\n",
       " 8.05579,\n",
       " 0.95577,\n",
       " 0.0456,\n",
       " 0.0315,\n",
       " 0.04741,\n",
       " 0.01096,\n",
       " 6.44405,\n",
       " 0.04011,\n",
       " 12.2472,\n",
       " 1.51902,\n",
       " 0.21161,\n",
       " 0.07875,\n",
       " 0.0837,\n",
       " 8.64476,\n",
       " 6.39312,\n",
       " 2.37857,\n",
       " 0.67191,\n",
       " 0.05497,\n",
       " 1.23247,\n",
       " 0.03445,\n",
       " 0.13914,\n",
       " 5.66998,\n",
       " 0.21038,\n",
       " 0.15086,\n",
       " 0.2896,\n",
       " 0.20746,\n",
       " 0.06047,\n",
       " 0.0187,\n",
       " 24.8017,\n",
       " 0.30347,\n",
       " 0.52693,\n",
       " 2.44953,\n",
       " 7.02259,\n",
       " 2.14918,\n",
       " 0.51183,\n",
       " 0.00632,\n",
       " 0.03359,\n",
       " 0.03768,\n",
       " 0.17142,\n",
       " 0.17004,\n",
       " 0.10959,\n",
       " 0.03427,\n",
       " 0.28392,\n",
       " 0.44178,\n",
       " 0.05083,\n",
       " 0.06127,\n",
       " 0.08387,\n",
       " 2.01019,\n",
       " 2.63548,\n",
       " 5.20177,\n",
       " 0.06417,\n",
       " 0.79041,\n",
       " 0.12579,\n",
       " 0.46296,\n",
       " 0.10574,\n",
       " 0.13587,\n",
       " 0.3494,\n",
       " 0.03659,\n",
       " 5.70818,\n",
       " 0.03049,\n",
       " 9.59571,\n",
       " 0.7857,\n",
       " 1.6566,\n",
       " 5.29305,\n",
       " 0.01778,\n",
       " 73.5341,\n",
       " 6.53876,\n",
       " 0.33147,\n",
       " 9.72418,\n",
       " 4.42228,\n",
       " 1.41385,\n",
       " 0.09103,\n",
       " 0.12204,\n",
       " 0.17899,\n",
       " 0.06588,\n",
       " 0.53412,\n",
       " 0.01709,\n",
       " 0.11432,\n",
       " 0.38214,\n",
       " 11.0874,\n",
       " 0.10084,\n",
       " 0.29819,\n",
       " 11.9511,\n",
       " 0.0536,\n",
       " 2.33099,\n",
       " 13.5222,\n",
       " 0.09604,\n",
       " 0.1146,\n",
       " 5.82401,\n",
       " 0.06151,\n",
       " 0.0459,\n",
       " 0.10153,\n",
       " 3.67367,\n",
       " 0.49298,\n",
       " 16.8118,\n",
       " 0.12744,\n",
       " 0.06905,\n",
       " 0.06724,\n",
       " 37.6619,\n",
       " 0.01501,\n",
       " 12.8023,\n",
       " 0.05372,\n",
       " 0.61154,\n",
       " 0.40202,\n",
       " 0.03466,\n",
       " 6.96215,\n",
       " 0.0686,\n",
       " 5.82115,\n",
       " 0.1403,\n",
       " 0.10612,\n",
       " 8.20058,\n",
       " 0.41238,\n",
       " 0.27957,\n",
       " 11.1081,\n",
       " 0.09299,\n",
       " 6.71772,\n",
       " 0.10008,\n",
       " 0.08187,\n",
       " 0.19802,\n",
       " 0.09065,\n",
       " 0.35809,\n",
       " 3.32105,\n",
       " 3.67822,\n",
       " 15.8603,\n",
       " 4.66883,\n",
       " 0.14231,\n",
       " 0.06129,\n",
       " 4.03841,\n",
       " 10.233,\n",
       " 0.11027,\n",
       " 5.09017,\n",
       " 8.49213,\n",
       " 0.04819,\n",
       " 1.20742,\n",
       " 0.04301,\n",
       " 15.1772,\n",
       " 9.96654,\n",
       " 4.87141,\n",
       " 0.3692,\n",
       " 0.0566,\n",
       " 0.57529,\n",
       " 0.17783,\n",
       " 0.11329,\n",
       " 0.02543,\n",
       " 0.22438,\n",
       " 0.05059,\n",
       " 0.06899,\n",
       " 51.1358,\n",
       " 11.5779,\n",
       " 0.04337,\n",
       " 0.77299,\n",
       " 3.8497,\n",
       " 0.06263,\n",
       " 0.21719,\n",
       " 0.01501,\n",
       " 18.811,\n",
       " 22.5971,\n",
       " 0.20608,\n",
       " 4.26131,\n",
       " 1.00245,\n",
       " 1.61282,\n",
       " 0.05789,\n",
       " 0.0351,\n",
       " 0.43571,\n",
       " 0.05302,\n",
       " 0.1676,\n",
       " 24.3938,\n",
       " 0.01301,\n",
       " 0.06162,\n",
       " 4.22239,\n",
       " 1.80028,\n",
       " 0.04684,\n",
       " 0.12816,\n",
       " 0.19539,\n",
       " 0.62356,\n",
       " 0.537,\n",
       " 0.12802,\n",
       " 0.10659,\n",
       " 6.80117,\n",
       " 0.05188,\n",
       " 0.02763,\n",
       " 19.6091,\n",
       " 9.33889,\n",
       " 0.12269,\n",
       " 0.08199,\n",
       " 0.08873,\n",
       " 0.13158,\n",
       " 0.05425,\n",
       " 1.12658,\n",
       " 0.03578,\n",
       " 0.06617,\n",
       " 0.05735,\n",
       " 0.19186,\n",
       " 0.22489,\n",
       " 0.5405,\n",
       " 0.13058,\n",
       " 6.28807,\n",
       " 0.23912,\n",
       " 0.7842,\n",
       " 3.77498,\n",
       " 0.37578,\n",
       " 1.22358,\n",
       " 0.1,\n",
       " 0.07022,\n",
       " 0.2498,\n",
       " 1.25179,\n",
       " 9.92485,\n",
       " 0.7258,\n",
       " 0.14476,\n",
       " 0.84054,\n",
       " 18.0846,\n",
       " 0.08308,\n",
       " 0.08221,\n",
       " 20.0849,\n",
       " 0.15936,\n",
       " 0.07978,\n",
       " 0.52014,\n",
       " 0.09164,\n",
       " 0.11425]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "x=x_train[0].tolist()\n",
    "x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 27.5,  15.6,  19.8,  27. ,  13.1,  23.1,  33. ,  12.5,  50. ,\n",
       "        18.7,  21.7,  30.5,  26.4,  12.8,  14.9,  21.4,  14. ,  27.5,\n",
       "        22.6,  11. ,  28. ,  17.1,   7. ,  15.4,  20.7,  23.9,  19.6,\n",
       "        32.4,  21.7,  22.6,  31. ,  11.3,  19.5,  12.7,  18.9,  13.2,\n",
       "        33.4,  24. ,  19.5,  36.4,  14.1,  21.7,  42.3,  13.8,  14.3,\n",
       "        16.2,  22.8,  19.4,  13.4,  15.6,  36.5,  25. ,  27.9,  11.7,\n",
       "        21.9,   7. ,  20.1,  24.1,  19.6,  18.6,  20.6,  18.4,  21.5,\n",
       "        20.6,  17.2,  35.2,  36.1,  28.4,  20.1,  11.5,  21.8,  50. ,\n",
       "        42.8,  19.9,  26.2,  35.4,  16.2,  24.3,  24.8,  24.5,  23.3,\n",
       "        22.9,  19.9,  14.4,  24.4,  28.7,  13.1,  18.2,  31.1,  24.7,\n",
       "        28.6,  20.3,  21.6,  22.9,  19.4,  31.6,  17.8,  13.8,  21. ,\n",
       "        13.1,  11.7,  18.4,  19.5,  18. ,  23.9,  38.7,  21.9,  12. ,\n",
       "        19.9,  22. ,  18.6,  27.1,  21.6,  10.4,  37.2,  21.4,  19.4,\n",
       "        22.5,  13. ,  16.5,  32.2,  22.2,  28.1,  30.1,  16. ,  50. ,\n",
       "        21. ,  11.8,   5. ,  22.2,  18.8,  44.8,  20.8,  28.7,  21.2,\n",
       "        14.5,  10.4,  50. ,  21.1,  44. ,  20.4,  13.4,  12.7,  29. ,\n",
       "        33.2,  11.9,  24.7,  13.1,  33.8,  23.7,  36. ,  19.9,  20.1,\n",
       "        14.3,  20.9,  15.2,  14.9,  23.1,  10.5,  28.5,  17.5,  50. ,\n",
       "         7.2,  18.3,  17.7,  23.7,  29.1,  12.3,  34.7,  26.6,  25. ,\n",
       "        25. ,  15.6,  14.1,  16.4,   5. ,  18.8,  17.4,  17.1,  18.9,\n",
       "        27.5,  20.5,  14.4,  15.6,  30.1,  13.8,  14.8,  23.3,  34.9,\n",
       "        11.9,  22. ,  16.1,  33.3,  10.2,  50. ,  19.3,  32. ,  34.9,\n",
       "        13.8,  13.3,  20.6,  16.6,  19. ,  15.2,  24.1,  23.1,  50. ,\n",
       "        35.1,  15.2,  19.7,   8.1,  29.6,  23.1,   8.3,  23. ,  50. ,\n",
       "        22.3,  14.2,  19.4,  31.5,  24. ,  34.9,  34.6,  19.3,  18.9,\n",
       "        22. ,  19.5,  18.5,  31.5,  22.2,  33.1,  20.3,  50. ,  16.1,\n",
       "        22.7,  18.9,  22.1,  29.8,  31.7,  13.6,  24.4,  20.3,  24.8,\n",
       "        23.7,  31.2,  12.1,  30.7,  21.5,  23.2,  32.9,   8.8,  50. ,\n",
       "        48.3,  17.1,  19.1,  17. ,  37.9,  28.4,  23.1,  39.8,  43.1,\n",
       "        30.1,  26.5,  37.6,  16.7,  22.8,  46.7,  27.9,  25. ,  17.8,\n",
       "        23.1,  32. ,  24.4,  23. ,  18.7,  22.3,  20. ,  21.2,  22.8,\n",
       "         7.2,  26.6,  36.2,  22.6,  10.9,  50. ,  10.8,  27.1,  50. ,\n",
       "        23.1,  19.4,  15.1,  33.2,  20.2,  24.4,  20.1,  13.5,  31.6,\n",
       "        24.5,  13.8,  20.5,  13.4,  32.5,  43.8,  25. ,  20.7,  26.7,\n",
       "        13.4,  20.8,   8.3,  12.7,  18.5,  46. ,  19.6,  14.6,  22.2,\n",
       "        16.1,  14.5,  21.9,  17.4,  18.2,   8.7,  15.4,  16.7,  23.8,\n",
       "        23.6,  41.7,  17.5,  22. ,  23.9,  16.8,  23.9,  22. ,  15. ,\n",
       "         9.7,  20.5,  18.4,  21.7,  22.4,  22.4,  24.5,  17.9,   7.4,\n",
       "        17.6,  22.6,  21. ,  13.5,  22. ,  48.5,  20. ,  28.7,  23.8,\n",
       "        10.5,  32.7,  17.2,  16.8,  23.8,  22.6,  20.9,  23.4,  27.5,\n",
       "        24.3,  19.8,  20.6,  20. ,  22.5,  30.8,  15. ,   9.5,  21.2,\n",
       "        21.7,  19.7,  21.2,  24.6,  15.3,  45.4,  19.3,  26.6,  24.6,\n",
       "        15. ,  43.5,  20.4,  14.9,  21.2,  17.5,  19. ,  19.3,  41.3,\n",
       "        33.1,  23.2,  13.3,  13.6,  12.6,  18.2,  19.3,  13.9,   7.2,\n",
       "        26.4,  29.6,   8.8,  24.7,  29.1,  48.8,  22.8,  23. ])"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "##for one feature\n",
    "def cost_fn(m,b,y,x):                \n",
    "    cost = 0.0\n",
    "    for i in range(len(y)):\n",
    "        cost+= int((y[i] - (m*x[i] + b))**2)\n",
    "    return(cost)\n",
    "\n",
    "def gradient_descent(x, y, learning_rate, b_current, m_current):\n",
    "    m_slope = 0\n",
    "    b_slope = 0\n",
    "    N = len(y)\n",
    "    for i in range(N):\n",
    "        b_slope += -(2/N)     * (y[i] - m_current* x[i] - b_current)\n",
    "        m_slope += -(2/N) * (y[i] - m_current* x[i] - b_current)*x[i]\n",
    "   \n",
    "    new_m = m_current - learning_rate * m_slope\n",
    "    new_b = b_current - learning_rate* b_slope\n",
    "    return(new_m,new_b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def run_fn():\n",
    "    b = 0   #initial value before updation\n",
    "    m = 0\n",
    "    learning_rate = 0.01\n",
    "    num_iterations = 1000\n",
    "    x = x_train[0].tolist()\n",
    "    y = y_train\n",
    "    cost_arr = np.array([0])\n",
    "    num_arr = np.array([0])\n",
    "    print('Start : ', cost_fn(m,b,y,x) )\n",
    "    for i in range(num_iterations):\n",
    "        m,b = gradient_descent(x, y, learning_rate, b, m)\n",
    "        cost_arr = np.append(cost_arr,cost_fn(m,b,y,x))\n",
    "        num_arr = np.append(num_arr,i)\n",
    "    plt.plot(cost_arr,num_arr)\n",
    "    plt.show()\n",
    "   \n",
    "    print (\"FINAL: \", cost_fn(m,b,y,x))\n",
    "    final_b, final_m = gradient_descent(x,y, learning_rate, b,m)\n",
    "    print('cost: ',cost_arr,'b: ',b,'m: ',m)\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start :  244416.0\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYAAAAD8CAYAAAB+UHOxAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x116c4be48>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FINAL:  29871.0\n",
      "cost:  [      0.  234817.  226120. ...,   29871.   29871.   29871.] b:  24.1908647134 m:  -0.395739548896\n"
     ]
    }
   ],
   "source": [
    "run_fn()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#for n features\n",
    "\n",
    "def gradient_descent(x, y, learning_rate, b_current, m_current):\n",
    "    m_slope = 0\n",
    "    b_slope = 0\n",
    "    N = len(y)\n",
    "    #w = w-(w.T.dot(X)-y)*X\n",
    "    for i in range(N):\n",
    "        b_slope += -(2/N) * (y[i] - m_current* x[i] - b_current)\n",
    "        m_slope += -(2/N) * (y[i] - m_current* x[i] - b_current)*x[i]\n",
    "   \n",
    "    new_m = m_current - learning_rate * m_slope\n",
    "    new_b = b_current - learning_rate* b_slope\n",
    "    return(new_m,new_b)\n",
    "\n",
    "def cost_fn(m,b,y,x):                \n",
    "    cost = []\n",
    "    c=0\n",
    "    for j in range(13):\n",
    "        for i in range(len(y)):\n",
    "            cost.append( int((y[i] - (m[j]*x[i][j] + b[j]))**2))\n",
    "    for j in range(13):\n",
    "        c+=cost[i]\n",
    "    return(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def run_fn():\n",
    "       #initial value before updation \n",
    "    m = []\n",
    "    x = np.zeros((len(x_train),13))\n",
    "    learning_rate = 0.001\n",
    "    num_iterations = 525\n",
    "    \n",
    "    y = y_train\n",
    "    \n",
    "#     print('Start : ', cost_fn(m,b,y,x) )\n",
    "    \n",
    "    for i in range(num_iterations):\n",
    "#         print(x_train[:][i].tolist())\n",
    "        a , b1 = gradient_descent(x_train[:][i].tolist(), y, learning_rate, 0, 0)\n",
    "#         np.append(cost_arr,cost_fn(m,b,y,x))\n",
    "#         np.append(num_arr,i)\n",
    "        np.append(m,a)\n",
    "        np.append(b,b1)\n",
    "    cost_fn(m,b,y,x)  \n",
    "#     plt.plot(cost_arr,num_arr)\n",
    "#     plt.show()\n",
    "   \n",
    "    print (\"FINAL: \", cost_fn(m,b,y,x))\n",
    "    final_b, final_m = gradient_descent(x,y, learning_rate, b,m)\n",
    "    print('cost: ',cost_arr,'b: ',b,'m: ',m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "13",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mget_loc\u001b[0;34m(self, key, method, tolerance)\u001b[0m\n\u001b[1;32m   2441\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2442\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2443\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.Int64HashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.Int64HashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 13",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-69-5fb168f8e0cf>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mrun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-68-d8ec420ae1ea>\u001b[0m in \u001b[0;36mrun_fn\u001b[0;34m()\u001b[0m\n\u001b[1;32m     17\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_iterations\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;31m#         print(x_train[:][i].tolist())\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m         \u001b[0ma\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0mb1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgradient_descent\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlearning_rate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m \u001b[0;31m#         np.append(cost_arr,cost_fn(m,b,y,x))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;31m#         np.append(num_arr,i)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   1962\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_getitem_multilevel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1963\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1964\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_getitem_column\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1965\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1966\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_getitem_column\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m_getitem_column\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   1969\u001b[0m         \u001b[0;31m# get column\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1970\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_unique\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1971\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_item_cache\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1972\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1973\u001b[0m         \u001b[0;31m# duplicate columns & possible reduce dimensionality\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36m_get_item_cache\u001b[0;34m(self, item)\u001b[0m\n\u001b[1;32m   1643\u001b[0m         \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcache\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1644\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mres\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1645\u001b[0;31m             \u001b[0mvalues\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1646\u001b[0m             \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_box_item_values\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalues\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1647\u001b[0m             \u001b[0mcache\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mres\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/pandas/core/internals.py\u001b[0m in \u001b[0;36mget\u001b[0;34m(self, item, fastpath)\u001b[0m\n\u001b[1;32m   3588\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3589\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misnull\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3590\u001b[0;31m                 \u001b[0mloc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3591\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3592\u001b[0m                 \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0misnull\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mget_loc\u001b[0;34m(self, key, method, tolerance)\u001b[0m\n\u001b[1;32m   2442\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2443\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2444\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_cast_indexer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2445\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2446\u001b[0m         \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_indexer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtolerance\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtolerance\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.Int64HashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.Int64HashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 13"
     ]
    }
   ],
   "source": [
    "run_fn()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5.70818"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_new = x_train[0].tolist()[2]\n",
    "x_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "404\n"
     ]
    }
   ],
   "source": [
    "print(len(x_train[:][1].tolist()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
